{\rtf1\ansi\ansicpg1252\cocoartf1561\cocoasubrtf400
{\fonttbl\f0\fnil\fcharset0 HelveticaNeue;}
{\colortbl;\red255\green255\blue255;\red53\green53\blue53;\red220\green161\blue13;}
{\*\expandedcolortbl;;\cssrgb\c27059\c27059\c27059;\cssrgb\c89412\c68627\c3922;}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid1}
{\list\listtemplateid2\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid101\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid2}
{\list\listtemplateid3\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid201\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid3}
{\list\listtemplateid4\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid301\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid4}
{\list\listtemplateid5\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid401\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid402\'01\'01;}{\levelnumbers\'01;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid403\'01\'02;}{\levelnumbers\'01;}\fi-360\li2160\lin2160 }{\listname ;}\listid5}
{\list\listtemplateid6\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid501\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid502\'01\'01;}{\levelnumbers\'01;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid503\'01\'02;}{\levelnumbers\'01;}\fi-360\li2160\lin2160 }{\listname ;}\listid6}
{\list\listtemplateid7\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid601\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid602\'01\'01;}{\levelnumbers\'01;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid603\'01\'02;}{\levelnumbers\'01;}\fi-360\li2160\lin2160 }{\listname ;}\listid7}
{\list\listtemplateid8\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid701\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid702\'01\'01;}{\levelnumbers\'01;}\fi-360\li1440\lin1440 }{\listname ;}\listid8}
{\list\listtemplateid9\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid801\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid802\'01\'01;}{\levelnumbers\'01;}\fi-360\li1440\lin1440 }{\listname ;}\listid9}
{\list\listtemplateid10\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid901\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid902\'01\'01;}{\levelnumbers\'01;}\fi-360\li1440\lin1440 }{\listname ;}\listid10}
{\list\listtemplateid11\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid1001\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid1002\'01\'01;}{\levelnumbers\'01;}\fi-360\li1440\lin1440 }{\listname ;}\listid11}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}{\listoverride\listid2\listoverridecount0\ls2}{\listoverride\listid3\listoverridecount0\ls3}{\listoverride\listid4\listoverridecount0\ls4}{\listoverride\listid5\listoverridecount0\ls5}{\listoverride\listid6\listoverridecount0\ls6}{\listoverride\listid7\listoverridecount0\ls7}{\listoverride\listid8\listoverridecount0\ls8}{\listoverride\listid9\listoverridecount0\ls9}{\listoverride\listid10\listoverridecount0\ls10}{\listoverride\listid11\listoverridecount0\ls11}}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\partightenfactor0

\f0\b\fs42 \cf2 \expnd0\expndtw0\kerning0
Data Science
\b0 \
\pard\pardeftab720\partightenfactor0

\fs26 \cf2 \
\pard\pardeftab720\partightenfactor0

\b \cf2 p-value 
\b0 : probability of obtaining a sample at least as extreme as ours if the null hypothesis were true.\
\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls1\ilvl0
\fs24 \cf2 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Cover the basic distributions - Bayesian, Gaussian, binomial, Poisson, chi-squared, exponential (these latter two are both just special cases of gamma).\'a0\
\ls1\ilvl0\kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Confidence intervals, summary statistics, p values, and moments of distributions. The central limit theorem.\
\ls1\ilvl0\kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Hypothesis testing using both parametric and nonparametric approaches. Bootstrapping, permutation tests, and Monte Carlo simulations.\'a0\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls1\ilvl0\cf3 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}{\field{\*\fldinst{HYPERLINK "https://www.coursera.org/learn/biostatistics"}}{\fldrslt \expnd0\expndtw0\kerning0
\ul Mathematical Biostatistics Bootcamp}}\cf2 \expnd0\expndtw0\kerning0
\'a0for better understanding of distributions and hypothesis testing.\'a0\
\pard\pardeftab720\partightenfactor0
\cf2 \
\pard\pardeftab720\partightenfactor0

\b\fs44 \cf2 Machine Learning
\b0\fs40 \'a0
\fs44 \
\pard\pardeftab720\partightenfactor0

\fs28 \cf2 \
\pard\pardeftab720\partightenfactor0

\b\fs32 \cf2 \ul \ulc2 Supervised \ulnone :\'a0
\b0 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls2\ilvl0
\fs24 \cf2 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Regression (continuous valued output)\'a0 A\'a0
\b regression
\b0 \'a0model predicts continuous values. For example, regression models make predictions that answer questions like the following:\
\pard\pardeftab720\partightenfactor0
\cf2 	What is the value of a house in California?\
	What is the probability that a user will click on this ad?\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls3\ilvl0\cf2 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Classification (Discrete valued output) A\'a0
\b classification
\b0 \'a0model predicts discrete values. For example, classification models make predictions that answer questions like the following:\
\pard\pardeftab720\partightenfactor0
\cf2 	Is a given email message spam or not spam?\
	Is this an image of a dog, a cat, or a hamster\
Training vs testing data set(the more data the better for both) -> If the training data is less, we can use cross validation.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls4\ilvl0\cf2 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
SVM to deal with infinite number of features.\
\pard\pardeftab720\partightenfactor0
\cf2 	
\b \ul Linear Regression\ulnone \'a0
\b0 \
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\partightenfactor0
\ls5\ilvl1\cf2 \kerning1\expnd0\expndtw0 {\listtext	1	}\expnd0\expndtw0\kerning0
Model and cost function\'a0\
\ls5\ilvl1\kerning1\expnd0\expndtw0 {\listtext	2	}\expnd0\expndtw0\kerning0
Gradient descent (need to choose alpha, needs many iterations, works for large n) Feature scaling (n) :\
\pard\tx1660\tx2160\pardeftab720\li2160\fi-2160\partightenfactor0
\ls5\ilvl2\cf2 \kerning1\expnd0\expndtw0 {\listtext	1	}\expnd0\expndtw0\kerning0
(between -1 <x<1) to converge quickly for gradient descent\
\ls5\ilvl2\kerning1\expnd0\expndtw0 {\listtext	2	}\expnd0\expndtw0\kerning0
mean normalization.\
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\partightenfactor0
\ls5\ilvl1\cf2 \kerning1\expnd0\expndtw0 {\listtext	3	}\expnd0\expndtw0\kerning0
Normal Equation = (X^TX)^-1 X^T y (no need of alpha ,don\'92t need to iterate, slow if n is large < 10,000)\'a0\
\pard\tx1660\tx2160\pardeftab720\li2160\fi-2160\partightenfactor0
\ls5\ilvl2\cf2 \kerning1\expnd0\expndtw0 {\listtext	1	}\expnd0\expndtw0\kerning0
if X^TX is non invertible -> check for redundant features, too many features -delete, use regularization.\'a0\
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\partightenfactor0
\ls5\ilvl1\cf2 \kerning1\expnd0\expndtw0 {\listtext	4	}\expnd0\expndtw0\kerning0
Multivariate Linear Regression\
\ls5\ilvl1\kerning1\expnd0\expndtw0 {\listtext	5	}\expnd0\expndtw0\kerning0
Multiple features\
\ls5\ilvl1\kerning1\expnd0\expndtw0 {\listtext	6	}\expnd0\expndtw0\kerning0
Polynomial regression\
\pard\pardeftab720\partightenfactor0
\cf2 \
\pard\pardeftab720\partightenfactor0

\b \cf2 \'a0\ul Logistic Regression
\b0 \ulnone \
\
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\partightenfactor0
\ls6\ilvl1\cf2 \kerning1\expnd0\expndtw0 {\listtext	1	}\expnd0\expndtw0\kerning0
Higher order polynomial functions\'a0\
\ls6\ilvl1\kerning1\expnd0\expndtw0 {\listtext	2	}\expnd0\expndtw0\kerning0
multi class classification : one-vs-all. Evaluate models using\'a0\
\pard\tx1660\tx2160\pardeftab720\li2160\fi-2160\partightenfactor0
\ls6\ilvl2\cf2 \kerning1\expnd0\expndtw0 {\listtext	1	}\expnd0\expndtw0\kerning0
accuracy = Fraction of prediction we got right = TP+TN/TP+FP+FN+TN\
\ls6\ilvl2\kerning1\expnd0\expndtw0 {\listtext	2	}\expnd0\expndtw0\kerning0
precision = True positive / all positive predictions = tp/tp+fp\
\ls6\ilvl2\kerning1\expnd0\expndtw0 {\listtext	3	}\expnd0\expndtw0\kerning0
recall = True positive / all actual positives = tp/tp+fn\
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\partightenfactor0
\ls6\ilvl1\cf2 \kerning1\expnd0\expndtw0 {\listtext	3	}\expnd0\expndtw0\kerning0
How to avoid Overfitting ?\'a0\
\pard\pardeftab720\partightenfactor0

\b \cf2 Regularization
\b0 \
\pard\tx1660\tx2160\pardeftab720\li2160\fi-2160\partightenfactor0
\ls7\ilvl2\cf2 \kerning1\expnd0\expndtw0 {\listtext	1	}\expnd0\expndtw0\kerning0
Early stopping before converging\
\ls7\ilvl2\kerning1\expnd0\expndtw0 {\listtext	2	}\expnd0\expndtw0\kerning0
penalizing the parameters (logistic and linear)\
\ls7\ilvl2\kerning1\expnd0\expndtw0 {\listtext	3	}\expnd0\expndtw0\kerning0
L1\'a0\
\ls7\ilvl2\kerning1\expnd0\expndtw0 {\listtext	4	}\expnd0\expndtw0\kerning0
L2\'a0\
\ls7\ilvl2\kerning1\expnd0\expndtw0 {\listtext	5	}\expnd0\expndtw0\kerning0
Prediction Bias ? Average of predictions = Average of observations.\
\pard\pardeftab720\partightenfactor0

\fs28 \cf2 \
\pard\pardeftab720\partightenfactor0

\b \cf2 Linear vs Logistic Regression
\b0 \
\
\pard\pardeftab720\partightenfactor0

\fs24 \cf2 \
\pard\pardeftab720\partightenfactor0

\b\fs30 \cf2 Debugging learning algorithms:
\b0 \
\pard\pardeftab720\partightenfactor0

\b\fs24 \cf2 Regularized linear regression
\b0 , during testing hypothesis on new set of samples, there is large errors in predictions, what to do?\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls8\ilvl0\cf2 \kerning1\expnd0\expndtw0 {\listtext	1	}\expnd0\expndtw0\kerning0
High variance (overfit):\'a0\
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\partightenfactor0
\ls8\ilvl1\cf2 \kerning1\expnd0\expndtw0 {\listtext	1	}\expnd0\expndtw0\kerning0
Get more training examples\
\ls8\ilvl1\kerning1\expnd0\expndtw0 {\listtext	2	}\expnd0\expndtw0\kerning0
try smaller sets of features\'a0\
\ls8\ilvl1\kerning1\expnd0\expndtw0 {\listtext	3	}\expnd0\expndtw0\kerning0
try increasing lambda\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls8\ilvl0\cf2 \kerning1\expnd0\expndtw0 {\listtext	2	}\expnd0\expndtw0\kerning0
High bias (underfit):\
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\partightenfactor0
\ls8\ilvl1\cf2 \kerning1\expnd0\expndtw0 {\listtext	1	}\expnd0\expndtw0\kerning0
try additional sets of features\
\ls8\ilvl1\kerning1\expnd0\expndtw0 {\listtext	2	}\expnd0\expndtw0\kerning0
try additional polynomial features\
\ls8\ilvl1\kerning1\expnd0\expndtw0 {\listtext	3	}\expnd0\expndtw0\kerning0
try decreasing lambda\
\pard\pardeftab720\partightenfactor0
\cf2 \
Evaluate a hypothesis -> cross validation.\
\pard\pardeftab720\partightenfactor0

\b \cf2 Bias(underfit) vs variance(overfit) :
\b0 \
Bias: Training and CV errors both will be high\
Variance: Training error is low and CV error is high\
\
Bias/variance as a function of Regularization\
\'a0\
\
\
Learning curves\
High bias: more training data won\'92t help\
\
High variance: more training likely to help.\
\
\

\b Recommended approach for new models
\b0 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls9\ilvl0\cf2 \kerning1\expnd0\expndtw0 {\listtext	1	}\expnd0\expndtw0\kerning0
start by simple algorithm, and test it on CV data\
\ls9\ilvl0\kerning1\expnd0\expndtw0 {\listtext	2	}\expnd0\expndtw0\kerning0
plot learning curves to decide if we need more data, more features\
\ls9\ilvl0\kerning1\expnd0\expndtw0 {\listtext	3	}\expnd0\expndtw0\kerning0
error analysis: manually examine examples(on CV set) and manually spot any systematic trend. e.g: stemming for spam filters.\
\ls9\ilvl0\kerning1\expnd0\expndtw0 {\listtext	4	}\expnd0\expndtw0\kerning0
error metrics for skewed classes : good algorithm has high precision & recall.\
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\partightenfactor0
\ls9\ilvl1\cf2 \kerning1\expnd0\expndtw0 {\listtext	1	}\expnd0\expndtw0\kerning0
Precision : TP/ TP+FP\
\ls9\ilvl1\kerning1\expnd0\expndtw0 {\listtext	2	}\expnd0\expndtw0\kerning0
Recall : TP/TP+FN if recall is zero -> data is skewed\
\ls9\ilvl1\kerning1\expnd0\expndtw0 {\listtext	3	}\expnd0\expndtw0\kerning0
F1 score: 2PR/P+R\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls9\ilvl0\cf2 \kerning1\expnd0\expndtw0 {\listtext	5	}\expnd0\expndtw0\kerning0
Large data sets - with lot of parameters , if human can predict y based on x?\
\pard\pardeftab720\partightenfactor0
\cf2 \
\pard\pardeftab720\partightenfactor0

\b\fs26 \cf2 \ul SVM (support vector machines)
\b0 \ulnone \
\pard\pardeftab720\partightenfactor0

\fs24 \cf2 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls10\ilvl0\cf2 \kerning1\expnd0\expndtw0 {\listtext	1	}\expnd0\expndtw0\kerning0
large margin classifier: maximize margin between closest support vectors.\
\ls10\ilvl0\kerning1\expnd0\expndtw0 {\listtext	2	}\expnd0\expndtw0\kerning0
SVM is deterministic and is faster for kernel space.\
\ls10\ilvl0\kerning1\expnd0\expndtw0 {\listtext	3	}\expnd0\expndtw0\kerning0
kernels:\'a0\
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\partightenfactor0
\ls10\ilvl1\cf2 \kerning1\expnd0\expndtw0 {\listtext	1	}\expnd0\expndtw0\kerning0
Linear kernels\
\ls10\ilvl1\kerning1\expnd0\expndtw0 {\listtext	2	}\expnd0\expndtw0\kerning0
Gaussian kernel : do perform feature scaling before using it.\
\pard\pardeftab720\partightenfactor0
\cf2 \
\pard\pardeftab720\partightenfactor0

\b\fs28 \cf2 Logistic Regression vs SVMs
\b0 \
\pard\pardeftab720\partightenfactor0

\fs24 \cf2 \
\
\pard\pardeftab720\partightenfactor0

\b\fs36 \cf2 \ul Unsupervised
\b0 \ulnone  :\'a0\
\pard\pardeftab720\partightenfactor0

\fs24 \cf2 Clustering, non-clustering.\
SVD to de clutter two voices in cocktail party problem.\
\pard\pardeftab720\partightenfactor0

\b \cf2 K-means algorithm
\b0 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls11\ilvl0\cf2 \kerning1\expnd0\expndtw0 {\listtext	1	}\expnd0\expndtw0\kerning0
choosing k - usually lower cost function for more k, but if k=3 > k=5 -> most likely k-mean was struck at bad local minima, so re-running will help.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls11\ilvl0
\b \cf2 \kerning1\expnd0\expndtw0 {\listtext	2	}\expnd0\expndtw0\kerning0
PCA - Principal component analysis
\b0 \'a0\
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\partightenfactor0
\ls11\ilvl1\cf2 \kerning1\expnd0\expndtw0 {\listtext	1	}\expnd0\expndtw0\kerning0
to reduce dimensions & speed up learning process\
\ls11\ilvl1\kerning1\expnd0\expndtw0 {\listtext	2	}\expnd0\expndtw0\kerning0
used to compress data & visualize high dimensionality data.\
\ls11\ilvl1\kerning1\expnd0\expndtw0 {\listtext	3	}\expnd0\expndtw0\kerning0
Don\'92t use PCA to prevent overfitting by reducing dimension, use regularization.\
\pard\pardeftab720\partightenfactor0
\cf2 \
\pard\pardeftab720\partightenfactor0

\b \cf2 Anomaly detection
\b0  p(x) < E\

\b \
Questions
\b0 \
1. How do you choose n grams? why choose 2 vs 5 and their advantages/disadvantages?\
2. Models performs poorly on test data why? what happens when n grams are 0 for testing data? --> borrow prior data from other similar data set/ generate synthetic data.\
3. What is CNN and logistic regression?\
}